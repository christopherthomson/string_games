{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String Games: Getting Started with Text Wrangling in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![A design in Cat's Cradle known as Ruapehu and Tongariro.](https://i.pinimg.com/736x/1e/2d/1a/1e2d1a97d166f62ec5b4579bf5128765.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get a sense of the key steps in writing a program to download web pages\n",
    "* Learn about Python tools for parsing web pages to extract text, linguistic information and named entities\n",
    "* Identify ways to apply these approaches to your own projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* About this notebook (5 mins)\n",
    "* Two typical scenarios (Get clean text for analysis, extract information for analysis)\n",
    "    - Scenario 1: Extracting text from HTML with BeautifulSoup (20 mins)\n",
    "    - Scenario 2: Extract POS and named entities with spaCy (20 mins)\n",
    "* Your own scenarios (30 mins)\n",
    "* Questions / wrap-up (5-10 mins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This type of workshop is often taught using interactive online notebooks. These make it easy to run small pieces of code and better understand each step in the program. They allow everyone to work in the same environment and get the same results (we hope!), and also allow text, headings, links etc to be interspersed in the code. \n",
    "\n",
    "You can also write Python programs with a text editor (such as Notepad++ for Windows, or TextWrangler for Mac), or various IDEs (integrated development environment programs). There are lots of ways to set up your computer to do Python programming, and the Programming Historian website has [a good guide](https://programminghistorian.org/en/lessons/introduction-and-installation) for Windows, Mac and Linux.\n",
    "\n",
    "If you want to use Jupyter notebooks (as we are here), I recommend you set this up, along with many of the Python libraries you might want, by installing the [Anaconda Community distribution](https://www.anaconda.com/distribution/).\n",
    "\n",
    "For the purposes of the workshop, this notebook is running through My Binder, a service that provides a server and programming environment to interact with the notebook online.\n",
    "\n",
    "##### Running code cells\n",
    "The notebook is made up of cells, which can be run individually or together. You must run them in order as the programs are sequential - later parts rely on the completion of earlier parts.\n",
    "\n",
    "To run a cell, you need to click on it to select it, then press the 'Run' button on the menu bar, or press Shift+Enter (the latter is much more convenient, once you get used to it). Working cell by cell helps break the program into pieces which can be understood more easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 1: Extract plain text from HTML with BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These lines starting with # are comments - they don't 'do' anything\n",
    "# Import 'requests', a library to help retrieve web pages\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We store a URL, giving it the variable name 'url'\n",
    "# This is our first example of a 'string variable' - the type of data we are mainly concerned with here.\n",
    "url = 'https://ucdh.github.io/scraping-garden-party/at-the-bay.html'\n",
    "\n",
    "# The requests library will retrieve various information about the page\n",
    "# By convention, we use 'r' to denote the object storing this information\n",
    "r = requests.get(url)\n",
    "\n",
    "# Print the status code and first 300 characters of the webpage text\n",
    "print(r.status_code)\n",
    "print(r.text[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1\n",
    "Go to [Geoff's mockup page about Katherine Mansfield's The Garden Party](https://ucdh.github.io/scraping-garden-party/at-the-bay.html) and choose a new story to download. In the cell below, do the following:\n",
    "\n",
    "* copy and modify the URL to be requested in order to download a different story of your choice.\n",
    "* use Shift+Enter to run the cell\n",
    "* modify the code so it prints the first 400 characters of the webpage text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BeautifulSoup can parse web page structure, much like a web browser does\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://ucdh.github.io/scraping-garden-party/at-the-bay.html'\n",
    "\n",
    "r = requests.get(url)\n",
    "\n",
    "# We create a BeautifulSoup object, using a parser library called 'lxml'\n",
    "# lxml isn't imported directly, but is a dependency (or requirement) here\n",
    "soup = BeautifulSoup(r.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select an element of the web page for display\n",
    "soup.title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like ```r```, ```soup``` is an object. Objects _bundle together_ many variables (properties of an object)  as well as methods (actions we can perform on the object).\n",
    "\n",
    "```soup.title``` gave us the page title. We can access other webpage properties you're probably familiar with, too. \n",
    "\n",
    "#### Task 2\n",
    "\n",
    "Change the last line in the cell above to display the following page elements (or 'tags', for short) stored in the ```soup``` object:\n",
    "\n",
    "- img\n",
    "- h1\n",
    "- p\n",
    "- a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we can get webpage elements / tags, but only the first example of any given tag. Learn more about BeautifulSoup [here](https://www.crummy.com/software/BeautifulSoup/bs4/doc/), such as the ```find_all()``` method that helps us extract all instances of a given element. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3\n",
    "Collect all the links stored in the ```soup```variable by replacing ```%``` character in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = soup.find_all('%')\n",
    "\n",
    "for link in links:\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, BeautifulSoup has a nice helper method to extract the clean text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text = soup.find(id='page-content').get_text() # Try removing the '.get_text()' bit to see the difference.\n",
    "clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can remove the output from a selected cell by going to Cell > Current Outputs > Clear in the menu bar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write the story to a file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's pretty easy to write the text to a file for later use. You could collect all the stories by looping over all the story page links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('story.txt', 'w') as f:\n",
    "    f.write(clean_text)\n",
    "    \n",
    "# this can be read as: open (or create) the file 'story.txt' in writing mode\n",
    "# assign the open file object to the variable f\n",
    "# write the contents of the 'clean_text' variable into f\n",
    "# (implied by the with statement) close the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 2: Extract parts of speech, named entities and text patterns with spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy is a library for 'natural language processing' - extracting linguistic features and other useful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This step assumes a spaCy language model (eg 'en_core_web_sm') has been installed\n",
    "# See https://spacy.io/usage/models#quickstart for details\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parts of Speech\n",
    "\n",
    "spaCy can identify parts of speech fairly accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text from 'At the Bay'. You can substitute other examples here\n",
    "# Just make sure you retain the triple quotes, which are used for multi-line strings and code comments\n",
    "sample_text = '''And now they had passed the fisherman’s hut, passed the charred-looking little whare where Leila the milk-girl lived with her old Gran.\n",
    "The sheep strayed over a yellow swamp and Wag, the sheep-dog, padded after, rounded them up and headed them for the steeper, narrower rocky pass that led out of Crescent Bay and towards Daylight Cove.\n",
    "'''\n",
    "\n",
    "doc = nlp(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display each word (aka 'token') and its part of speech\n",
    "for token in doc:\n",
    "    print(token.pos_, token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the displacy library visualises the dependency tree, giving us a fancier view\n",
    "from spacy import displacy\n",
    "displacy.render(doc, style=\"dep\", options={'compact': True, 'distance': 100})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PS Did you spot any errors? These statistical models are good, but not perfect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3\n",
    "\n",
    "Choose another sentence or two containing some interesting words or names (or 'plausible distractors') to test on spaCy's model. Paste these into the variable ```sample_text``` above and re-run it, then re-run the two code cells that follow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting parts of speech from longer texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parts of speech are very useful for identifying linguistic features of interest. For example, we might extract adjectives from a whole story with the following code (and this could be scaled up to all stories in a collection, and beyond). Such techniques can be helpful for literary, discourse or content analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll make a new variable 'doc2', so as not to get confused!\n",
    "# Remember, 'clean_text' is the variable output from BeautifulSoup.\n",
    "doc2 = nlp(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# here we are using a loop to collect adjectives\n",
    "# it's just a more compact syntax, called a 'list comprehension'\n",
    "adjectives = [token.text for token in doc2 if token.pos_ == 'ADJ']\n",
    "freq = Counter(adjectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other Python libraries you should consider for this kind of linguistic analysis, notably the Natural Language Tool Kit (NLTK). For matching patterns, try spaCy's rule-based pattern matcher, and, for more fine-grained control, learn how to use regular expressions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4\n",
    "\n",
    "Select a different story by changing the ```url``` variable back in Scenario 1, and process it with BeautifulSoup so that it is stored in the variable ```clean_text```. Then re-run the previous three cells to see which adjectives are frequent in that story. This will test your memory and understanding of which cells are doing what!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting named entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find out a lot about objects in our text by examining the nouns and proper nouns. Even better, spaCy gives us a way to get noun phrases, what it calls \"noun chunks\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's go back to our earlier sample text, so we all have the same example\n",
    "\n",
    "sample_text = '''And now they had passed the fisherman’s hut, passed the charred-looking little whare where Leila the milk-girl lived with her old Gran.\n",
    "The sheep strayed over a yellow swamp and Wag, the sheep-dog, padded after, rounded them up and headed them for the steeper, narrower rocky pass that led out of Crescent Bay and towards Daylight Cove.\n",
    "'''\n",
    "\n",
    "doc3 = nlp(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in doc3.noun_chunks:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we can do better than this if we are interested in examining _entities_ such as people, places, organisations, dates or numbers.  spaCy's ```.ents``` method returns a list these entities, as in the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc3.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will count Wag the dog as a person.\n",
    "\n",
    "Fiction is a little more challenging for entity extraction, but for many types of text spaCy will do very well. Here is another example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://en.wikipedia.org/wiki/Katherine_Mansfield\n",
    "\n",
    "media_text = '''Kathleen Mansfield Murry (née Beauchamp; 14 October 1888 – 9 January 1923) was a prominent New Zealand modernist short story writer and poet who was born and brought up in colonial New Zealand and wrote under the pen name of Katherine Mansfield. \n",
    "At the age of 19, she left New Zealand and settled in England, where she became a friend of writers such as D. H. Lawrence and Virginia Woolf. \n",
    "Mansfield was diagnosed with extrapulmonary tuberculosis in 1917; the disease claimed her life at the age of 34.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc4 = nlp(media_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc4.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structuring your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful task is to extract this kind of information and store it in a spreadsheet. \n",
    "\n",
    "Let's get a bit more data. Say we want to count adjectives in every story in the collection; we could go about it like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, make a list of URLs to all the stories\n",
    "contents_url = 'https://ucdh.github.io/scraping-garden-party/'\n",
    "r = requests.get(contents_url)\n",
    "soup = BeautifulSoup(r.text)\n",
    "contents_list = soup.find_all('li')\n",
    "\n",
    "# next we use an 'accumulator pattern' \n",
    "# make an empty list, then add the URLs to it one by one\n",
    "story_urls = []\n",
    "\n",
    "for item in contents_list:\n",
    "    url_ending = item.a['href']\n",
    "    story_urls.append('https://ucdh.github.io' + url_ending)\n",
    "\n",
    "print(story_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll go through, story by story, and collect data about each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "\n",
    "for story_url in story_urls:\n",
    "    # create a variable to hold data about individual stories\n",
    "    story_data = {}\n",
    "    \n",
    "    r = requests.get(story_url)\n",
    "    soup = BeautifulSoup(r.text, 'lxml')\n",
    "    clean_text = soup.get_text()\n",
    "    \n",
    "    # collect the title, while tidying up a bit\n",
    "    story_data['story_title'] = soup.title.contents[0].split('|')[0]\n",
    "    \n",
    "    doc5 = nlp(clean_text)\n",
    "    \n",
    "    # collect adjective counts for 10 most common\n",
    "    adjectives = [token.text for token in doc5 if token.pos_ == 'ADJ']\n",
    "    adj_count = Counter(adjectives).most_common(10)\n",
    "    \n",
    "    story_length = doc5.__len__()\n",
    "    \n",
    "    story_data['story_length'] = story_length\n",
    "    \n",
    "    # add normalised counts for each word to our story_data variable\n",
    "    for word in adj_count:\n",
    "        story_data[word[0]] = round((word[1] * 10000) / story_length, 2)\n",
    "        \n",
    "        # or raw counts: \n",
    "        # story_data[word[0]] = word[1]\n",
    "        \n",
    "    # append our story data variable to the 'all_data' list before moving on to the next story\n",
    "    all_data.append(story_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the data for one story looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell\n",
    "all_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to organise our data into columns, and in many stories a lot of the adjectives won't appear, so those fields will be empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_headings = []\n",
    "\n",
    "for story in all_data:\n",
    "    \n",
    "    # get a list of adjectives for each story\n",
    "    headings = list(story.keys())\n",
    "    \n",
    "    # for each adjective, add it to our column headings if it's not there already\n",
    "    for heading in headings:\n",
    "        if heading not in col_headings:\n",
    "            col_headings.append(heading)\n",
    "            \n",
    "col_headings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving tabular data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSV files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write out tabular data using a standard Python library called ```csv```. Again we open a file, create an object and then instead of a string (text) we write our rows of data to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv import DictWriter\n",
    "\n",
    "# here we write to a 'tab separated file' which can be opened in Excel\n",
    "with open('datafile.tsv', 'w') as f:\n",
    "    fields = [col_headings]\n",
    "    writer = DictWriter(f, fieldnames=col_headings, delimiter='\\t')\n",
    "    writer.writeheader()\n",
    "    writer.writerows(all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where to from here?\n",
    "\n",
    "It's only worth investing time in learning to modify and write your own programs if you have a problem they can solve for you. Remember, it could take longer to write a program than to do the work another way.\n",
    "\n",
    "However, it's important to know that if you want to, you can collect a lot of interesting data from online sources or extract information from within texts.\n",
    "\n",
    "#### Task 5\n",
    "\n",
    "Think of a project you could try with the examples we've seen so far. You might look at sources such as:\n",
    "\n",
    "* [New Zealand Electronic Text Centre](http://nzetc.victoria.ac.nz/)\n",
    "* [DigitalNZ](https://digitalnz.org/)\n",
    "\n",
    "A great exercise is collecting funded projects and dollar amounts from the [NZ On Air funding decisions webpage](https://www.nzonair.govt.nz/search/?q=&search_type=decisions&selected_facets=funding_round%3ALatest+funding+round)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "Getting started is not too difficult. Here are some of the key steps and bits of code we've used today.\n",
    "\n",
    "##### Requests\n",
    "\n",
    "```r = requests.get(url)``` to request a web page's source\n",
    "\n",
    "Read the requests [documentation here](https://requests.kennethreitz.org/en/master/).\n",
    "\n",
    "##### BeautifulSoup\n",
    "\n",
    "```soup = BeautifulSoup(r.text, 'lxml')``` to parse the page\n",
    "\n",
    "Read the BeautifulSoup [documentation here](https://www.crummy.com/software/BeautifulSoup/bs4/doc/).\n",
    "\n",
    "##### spaCy\n",
    "\n",
    "Create a spaCy object:\n",
    "\n",
    "```text = \"load in some words\"```\n",
    "```doc = nlp(text)```\n",
    "\n",
    "Then access word attributes like Part of Speech tags with ```token.pos_``` or entities in the document with ```doc.ents```\n",
    "\n",
    "Read the spaCy [documentation here](https://spacy.io/usage)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
